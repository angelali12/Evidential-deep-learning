{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics## ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.tri as tri\n",
    "from scipy import stats\n",
    "from scipy.special import gamma as gamma_fun\n",
    "from scipy.integrate import odeint\n",
    "import scipy.special as spec\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import timeit\n",
    "\n",
    "\n",
    "from deep_bayes.models import DeepEvidentialModel, MCDropOutModel, SoftmaxModel, VAE\n",
    "from deep_bayes.settings import EVIDENTIAL_EPI, SOFTMAX_EPI, VAE_EPI, DROPOUT_EPI,EVIDENTIAL_HH, SOFTMAX_HH, VAE_HH, DROPOUT_HH\n",
    "from deep_bayes.training import train_online, train_online_vae, train_online_dropout, train_online_softmax\n",
    "from deep_bayes.losses import log_loss\n",
    "from deep_bayes.viz import plot_model_samples, plot_confusion_matrix\n",
    "from deep_bayes.diagnostics import accuracy, overconfidence, expected_calibration_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative model specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prior \n",
    "<p>Defines the prior probabilities over models $p(\\boldsymbol{m})$.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prior(batch_size, n_models, p_vals=None):\n",
    "    \"\"\"\n",
    "    Samples from the models' prior batch size times and converts to one-hot.\n",
    "    Assumes equal model priors.\n",
    "    ----------\n",
    "    \n",
    "    Arguments:\n",
    "    batch_size : int  -- the number of samples to draw from the prior\n",
    "    n_models   : int  -- the number of generative models in queston\n",
    "    ----------\n",
    "    \n",
    "    Returns:\n",
    "    m_true : np.ndarray of shape (batch_size, theta_dim) -- the samples batch of parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Equal priors, if nothign specified\n",
    "    if p_vals is None:\n",
    "        p_vals = [1/n_models] * n_models\n",
    "    m_idx = np.random.choice(n_models, size=batch_size, p=p_vals).astype(np.int32)\n",
    "    return m_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters priors\n",
    "<p>Defines how parameters are generated given a model $p(\\boldsymbol{\\theta}|\\boldsymbol{m})$.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIR_basic_prior(**args):\n",
    "    \n",
    "    theta = (\n",
    "        np.random.uniform(low=0.001, high=1.0), # beta\n",
    "        np.random.uniform(low=0.001, high=1.0), # gamma\n",
    "    )\n",
    "    return theta\n",
    "\n",
    "\n",
    "def SIR_vitalDynamics_prior(**args):\n",
    " \n",
    "    theta = (\n",
    "        np.random.uniform(low=0.001, high=0.1), # mu\n",
    "        np.random.uniform(low=0.001, high=0.1), # omega\n",
    "        np.random.uniform(low=0.001, high=1.0), # beta\n",
    "        np.random.uniform(low=0.001, high=1.0), # gamma\n",
    "    )\n",
    "    return theta\n",
    "\n",
    "\n",
    "def SIRS_basic_prior(**args):\n",
    "\n",
    "    theta = (\n",
    "        np.random.uniform(low=0.001, high=1.0), # beta\n",
    "        np.random.uniform(low=0.001, high=1.0), # gamma\n",
    "        np.random.uniform(low=0.025, high=0.1), # xi\n",
    "    )\n",
    "    return theta\n",
    "\n",
    "def SIRS_vitalDynamics_prior(**args):\n",
    "\n",
    "    theta = (\n",
    "        np.random.uniform(low=0.001, high=0.1), # mu\n",
    "        np.random.uniform(low=0.001, high=0.1), # omega\n",
    "        np.random.uniform(low=0.001, high=1.0), # beta\n",
    "        np.random.uniform(low=0.001, high=1.0), # gamma\n",
    "        np.random.uniform(low=0.025, high=0.1), # xi\n",
    "    )\n",
    "    return theta\n",
    "\n",
    "def SIRS_IncidenceRate_prior(**args):\n",
    "\n",
    "    theta = (\n",
    "        np.random.uniform(low=0.001, high=0.1), # mu\n",
    "        np.random.uniform(low=0.001, high=0.1), # omega\n",
    "        np.random.uniform(low=0.001, high=1.0), # gamma\n",
    "        np.random.uniform(low=0.025, high=0.1), # xi\n",
    "        np.random.uniform(low=0.01, high=5.0), # k\n",
    "        np.random.uniform(low=0.01, high=5.1), # alpha\n",
    "    )\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative models\n",
    "<p>Defines how the data are generated, that is, $\\boldsymbol{x}_{1:N} \\sim p(\\boldsymbol{x}| \\boldsymbol{\\theta},\\boldsymbol{m}).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIR_basic(params, n_obs, N=500, I0=1, R0=0, **args):\n",
    "    \n",
    "    # beta: infectious rate\n",
    "    # gamma: recovery rate\n",
    "    \n",
    "    beta, gamma = params\n",
    "    S0 = N - I0 - R0 # Initial susceptibles\n",
    "    \n",
    "    t = np.linspace(0, n_obs, n_obs)\n",
    "    \n",
    "    # differential equations.\n",
    "    def deriv(y, t, N, beta, gamma):\n",
    "        S, I, R = y\n",
    "        dS = -beta * S * I / N\n",
    "        dI = beta * S * I / N - gamma * I\n",
    "        dR = gamma * I\n",
    "        return dS, dI, dR\n",
    "    \n",
    "    # Initial conditions vector\n",
    "    y0 = S0, I0, R0\n",
    "    \n",
    "    # Integrate .\n",
    "    out = odeint(deriv, y0, t, args=(N, beta, gamma))\n",
    "    S, I, R = out.T\n",
    "    \n",
    "    return np.c_[S,I,R]\n",
    "\n",
    "\n",
    "def SIR_vitalDynamics(params, n_obs, N=500, I0=1, R0=0, **args):\n",
    "    \n",
    "    # mu: birth rate\n",
    "    # omega: death rate\n",
    "    # beta: infectious rate\n",
    "    # gamma: recovery rate\n",
    "    \n",
    "    mu, omega, beta, gamma = params\n",
    "    S0 = N - I0 - R0 # Initial susceptibles\n",
    "    \n",
    "    t = np.linspace(0, n_obs, n_obs)\n",
    "    \n",
    "    # differential equations.\n",
    "    def deriv(y, t, N, mu, omega, beta, gamma):\n",
    "        S, I, R = y\n",
    "        dS = mu*N - beta * S * I / N - omega*S\n",
    "        dI = beta * S * I / N - gamma * I - omega*I\n",
    "        dR = gamma * I - omega*R\n",
    "        return dS, dI, dR\n",
    "    \n",
    "    # Initial conditions vector\n",
    "    y0 = S0, I0, R0\n",
    "    \n",
    "    # Integrate \n",
    "    out = odeint(deriv, y0, t, args=(N, mu, omega, beta, gamma))\n",
    "    S, I, R = out.T\n",
    "    \n",
    "    return np.c_[S,I,R]\n",
    "\n",
    "def SIRS_basic(params, n_obs, N=500, I0=1, R0=0, **args):\n",
    "    \n",
    "    # beta: contact rate\n",
    "    # gamma: infectious rate\n",
    "    # xi: immunity loss rate\n",
    "    \n",
    "    beta, gamma, xi = params\n",
    "    S0 = N - I0 - R0 # Initial susceptibles\n",
    "    \n",
    "    t = np.linspace(0, n_obs, n_obs)\n",
    "    \n",
    "    # differential equations.\n",
    "    def deriv(y, t, N, beta, gamma, xi):\n",
    "        S, I, R = y\n",
    "        dS = -beta * S * I / N + xi*R\n",
    "        dI = beta * S * I / N - gamma * I\n",
    "        dR = gamma * I - xi*R\n",
    "        return dS, dI, dR\n",
    "    \n",
    "    # Initial conditions vector\n",
    "    y0 = S0, I0, R0\n",
    "    \n",
    "    # Integrate .\n",
    "    out = odeint(deriv, y0, t, args=(N, beta, gamma, xi))\n",
    "    S, I, R = out.T\n",
    "    \n",
    "    return np.c_[S,I,R]\n",
    "\n",
    "def SIRS_vitalDynamics(params, n_obs, N=500, I0=1, R0=0, **args):\n",
    "    \n",
    "    # mu: birth rate\n",
    "    # omega: death rate\n",
    "    # beta: infectious rate\n",
    "    # gamma: recovery rate\n",
    "    # xi: immunity loss \n",
    "    \n",
    "    mu, omega, beta, gamma, xi = params\n",
    "    S0 = N - I0 - R0 # Initial susceptibles\n",
    "    \n",
    "    t = np.linspace(0, n_obs, n_obs)\n",
    "    \n",
    "    # differential equations.\n",
    "    def deriv(y, t, N, mu, omega, beta, gamma, xi):\n",
    "        S, I, R = y\n",
    "        dS = mu*N - beta * S * I / N + xi*R - omega*S\n",
    "        dI = beta * S * I / N - gamma * I - omega*I\n",
    "        dR = gamma * I - xi*R - omega*R\n",
    "        return dS, dI, dR\n",
    "    \n",
    "    # Initial conditions vector\n",
    "    y0 = S0, I0, R0\n",
    "    \n",
    "    # Integrate \n",
    "    out = odeint(deriv, y0, t, args=(N, mu, omega, beta, gamma, xi))\n",
    "    S, I, R = out.T\n",
    "    \n",
    "    return np.c_[S,I,R]\n",
    "\n",
    "def SIRS_IncidenceRate(params, n_obs, N=500, I0=1, R0=0, **args):\n",
    "    \n",
    "    # mu: birth rate\n",
    "    # omega: death rate\n",
    "    # gamma: recovery rate\n",
    "    # xi: immunity loss \n",
    "    # k: infectious force\n",
    "    # alpha: inhibition effect\n",
    "    \n",
    "    mu, omega, gamma, xi, k, alpha = params\n",
    "    S0 = N - I0 - R0 # Initial susceptibles\n",
    "    \n",
    "    t = np.linspace(0, n_obs, n_obs)\n",
    "    \n",
    "    # differential equations.\n",
    "    def deriv(y, t, N, mu, omega, gamma, xi, k, alpha):\n",
    "        S, I, R = y\n",
    "        dS = mu*N - S*(k*I**2)/(1+alpha*I**2) + xi*R - omega*S\n",
    "        dI = S*(k*I**2)/(1+alpha*I**2) - gamma * I - omega*I\n",
    "        dR = gamma * I - xi*R - omega*R\n",
    "        return dS, dI, dR\n",
    "    \n",
    "    # Initial conditions vector\n",
    "    y0 = S0, I0, R0\n",
    "    \n",
    "    # Integrate \n",
    "    out = odeint(deriv, y0, t, args=(N, mu, omega, gamma, xi, k, alpha))\n",
    "    S, I, R = out.T\n",
    "    \n",
    "    return np.c_[S,I,R]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator\n",
    "<p>Define the algorithm:</p>\n",
    "1. <strong>for</strong> $i = 1,...,B$<br>\n",
    "2. &emsp;$\\boldsymbol{m}^{(i)} \\sim p(\\boldsymbol{m})$<br>\n",
    "3. &emsp;$\\boldsymbol{\\theta}^{(i)} \\sim p(\\boldsymbol{\\theta}|\\boldsymbol{m}^{(i)})$<br>\n",
    "4. &emsp;$\\boldsymbol{x}_{1:N}^{(i)} \\sim p(\\boldsymbol{x}|\\boldsymbol{\\theta}^{(i)},\\boldsymbol{m}^{(i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, model_prior, param_priors, forward_models, alpha_smooth=None,\n",
    "                   n_obs=None, n_obs_min=100, n_obs_max=500, to_tensor=True, **args):\n",
    "    \"\"\"\n",
    "    Runs the forward model 'batch_size' times by first sampling fromt the prior\n",
    "    theta ~ p(theta) and running x ~ p(x|theta).\n",
    "    ----------\n",
    "    \n",
    "    Arguments:\n",
    "    batch_size     : int -- the number of samples to draw from the prior\n",
    "    model_prior    : callable -- the prior which returns a sample of model indices\n",
    "    param_priors   : list of callables -- a list with model prior samplers\n",
    "    forward_models : list of generative models g(theta) -- a list of generatibve models\n",
    "    alpha_smooth   : float or None -- the label smoothing hyperparameter\n",
    "    n_obs          : int -- the numebr of observations to draw from p(x|theta)\n",
    "    n_obs_min      : int -- used when n_obs is None, draws n_obs ~ U(n_obs_min, n_obs_max)\n",
    "    n_obs_max      : int -- used when n_obs is None, draws n_obs ~ U(n_obs_min, n_obs_max)\n",
    "    to_tensor      : boolean -- converts theta and x to tensors if True\n",
    "    ----------\n",
    "    \n",
    "    Output:\n",
    "    m     : tf.Tensor or np.ndarray of shape (batch_size, n_models) - the one-hot encoded version of the models\n",
    "    theta : tf.Tensor or np.ndarray of shape (batch_size, theta_dim) - the data gen parameters \n",
    "    x     : tf.Tensor of np.ndarray of shape (batch_size, n_obs, x_dim)  - the generated data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample from model prior\n",
    "    # m_indices is an np.int32 array of model indices  \n",
    "    n_models = len(forward_models)\n",
    "    m_indices = model_prior(batch_size, n_models)\n",
    "    \n",
    "    # Sample N, if not specified\n",
    "    if n_obs is None:\n",
    "        n_obs = np.random.randint(low=n_obs_min, high=n_obs_max+1)\n",
    "    \n",
    "    # For each model index, sample from prior and run generative model.\n",
    "    x = []\n",
    "    theta = []\n",
    "    for m_idx in m_indices:\n",
    "        \n",
    "        # Draw from model prior theta ~ p(theta|\n",
    "        theta_m = param_priors[m_idx]()\n",
    "        \n",
    "        # Generate data from x = g_m(theta) <=> x ~ p(x|theta,m)\n",
    "        x_m = forward_models[m_idx](theta_m, n_obs)\n",
    "        \n",
    "        # Store data and params\n",
    "        x.append(x_m)\n",
    "        theta.append(theta_m)\n",
    "    \n",
    "    # One-hot encode model indices\n",
    "    m = to_categorical(m_indices, num_classes=n_models)\n",
    "    \n",
    "    # Apply label smoothing, if specified\n",
    "    if alpha_smooth is not None:\n",
    "        m = m * (1 - alpha_smooth) + alpha_smooth / n_models\n",
    "    \n",
    "    # Convert to tensor, if specified \n",
    "    if to_tensor:\n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        m = tf.convert_to_tensor(m, dtype=tf.float32)\n",
    "    return {'m': m, 'theta': theta, 'x': x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_calibration(model, test_data_c, model_names, n_chunks=10, n_bins=15):\n",
    "    \n",
    "    # Compute probabilities and calibrations\n",
    "    preds = np.concatenate([model.predict(x, to_numpy=True)['m_probs'] \n",
    "                            for x in tf.split(test_data_c['x'], n_chunks, axis=0)], axis=0)\n",
    "    cal_errs, cal_probs = expected_calibration_error(test_data_c['m'], preds, n_bins=n_bins)\n",
    "    \n",
    "    # Plot calibration curves\n",
    "    f, axarr = plt.subplots(1, 5, figsize=(12, 3))\n",
    "    for i, ax in enumerate(axarr.flat):\n",
    "        ax.plot(cal_probs[i][0], cal_probs[i][1])\n",
    "        ax.plot(ax.get_xlim(), ax.get_xlim(), '--', color='black')\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.set_title(model_names[i])\n",
    "        ax.set_xlabel('Accuracy')\n",
    "        ax.set_ylabel('Confidence')\n",
    "        ax.set_xticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    f.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Training hyperparameters ---#\n",
    "ckpt_file = \"epidemiology_model_selection_vae\"\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "iterations_per_epoch = 100\n",
    "alpha_smooth = None\n",
    "n_obs_min = 50\n",
    "n_obs_max = 150\n",
    "n_obs = 100\n",
    "n_test = 500\n",
    "n_test_cal = 5000\n",
    "\n",
    "\n",
    "#--- Optimizer hyperparameters ---#\n",
    "starter_learning_rate = 0.0005\n",
    "global_step = tfe.Variable(0, dtype=tf.int32)\n",
    "decay_steps = 5000\n",
    "decay_rate = .9\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, \n",
    "                                           decay_steps, decay_rate, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "#--- Generative model hyperparameters ---#\n",
    "model_names = ['SIR','SIRS-VD', 'SIRS-IR']\n",
    "\n",
    "param_priors = [SIR_basic_prior, \n",
    "                SIRS_vitalDynamics_prior, \n",
    "                SIRS_IncidenceRate_prior]\n",
    "\n",
    "forward_models = [SIR_basic, \n",
    "                  SIRS_vitalDynamics, \n",
    "                  SIRS_IncidenceRate]\n",
    "\n",
    "\n",
    "data_gen = partial(data_generator, \n",
    "                   model_prior=model_prior, \n",
    "                   param_priors=param_priors,\n",
    "                   forward_models=forward_models,\n",
    "                   alpha_smooth=alpha_smooth,\n",
    "                   n_obs_min=n_obs_min,\n",
    "                   n_obs_max=n_obs_max,\n",
    "                   n_obs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 s, sys: 202 ms, total: 21.1 s\n",
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_data_c = data_gen(n_test_cal, n_obs=500)\n",
    "test_data = data_gen(n_test, n_obs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = VAE(VAE_HH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from ./checkpoints/epidemiology_model_selection_vae/ckpt-3\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(step=global_step, optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(checkpoint, './checkpoints/{}'.format(ckpt_file), max_to_keep=2)\n",
    "checkpoint.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/liangela/Desktop/BayesFlow-master/legacy/deep_bayes/models.py:738: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor's shape (128, 3) is not compatible with supplied shape [5, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-58247b2abfa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        \u001b[0mm_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                        \u001b[0mmodel_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                        figsize=(12, 8))\n\u001b[0m",
      "\u001b[0;32m~/Desktop/BayesFlow-master/legacy/deep_bayes/viz.py\u001b[0m in \u001b[0;36mplot_model_samples\u001b[0;34m(model, x_test, m_test, model_names, n_samples, figsize, fontsize, show, filename)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0mn_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mm_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m     \u001b[0mm_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/BayesFlow-master/legacy/deep_bayes/models.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, x, n_samples, to_numpy)\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         \u001b[0mm_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m         \u001b[0mm_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mto_numpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;31m# Eager execution on data tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    678\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1879\u001b[0m       \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m     \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m     \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m         trainable=True)\n\u001b[0m\u001b[1;32m   1018\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m       self.bias = self.add_weight(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    385\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# there is nothing to restore.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         checkpoint_initializer = self._preload_simple_restoration(\n\u001b[0;32m--> 644\u001b[0;31m             name=name, shape=shape)\n\u001b[0m\u001b[1;32m    645\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mcheckpoint_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_preload_simple_restoration\u001b[0;34m(self, name, shape)\u001b[0m\n\u001b[1;32m    709\u001b[0m         key=lambda restore: restore.checkpoint.restore_uid)\n\u001b[1;32m    710\u001b[0m     return CheckpointInitialValue(\n\u001b[0;32m--> 711\u001b[0;31m         checkpoint_position=checkpoint_position, shape=shape)\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_track_trackable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, checkpoint_position, shape)\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0;31m# We need to set the static shape information on the initializer if\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0;31m# possible so we don't get a variable with an unknown shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapped_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    961\u001b[0m       raise ValueError(\n\u001b[1;32m    962\u001b[0m           \u001b[0;34m\"Tensor's shape %s is not compatible with supplied shape %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m           (self.shape, shape))\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m   \u001b[0;31m# Methods not supported / implemented for Eager Tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor's shape (128, 3) is not compatible with supplied shape [5, 3]"
     ]
    }
   ],
   "source": [
    "#run and plot performance of untrained networks\n",
    "plot_model_samples(model, \n",
    "                       x_test=test_data['x'][:25], \n",
    "                       m_test=test_data['m'][:25], \n",
    "                       model_names=model_names,\n",
    "                       figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for ep in range(1, epochs+1):\n",
    "    with tqdm(total=iterations_per_epoch, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "        losses = train_online_vae(model=model, \n",
    "                              optimizer=optimizer, \n",
    "                              data_gen=data_gen, \n",
    "                              iterations=iterations_per_epoch,\n",
    "                              batch_size=batch_size,\n",
    "                              p_bar=p_bar,\n",
    "                              global_step=global_step)\n",
    "        \n",
    "        \n",
    "        # Plot stuff\n",
    "        m_pred = model(test_data['x'])['m_probs']\n",
    "        plot_confusion_matrix(m_pred.numpy().argmax(axis=1), test_data['m'], model_names, figsize=(8,4),normalize=True)\n",
    "        print(accuracy(test_data['m'], m_pred))\n",
    "\n",
    "        # Store checkpoint\n",
    "        manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data['x'][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data['m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pred = model(test_data['x'])['m_probs']\n",
    "plot_confusion_matrix(m_pred.numpy().argmax(axis=1), test_data['m'], model_names, figsize=(8,4),normalize=True)\n",
    "\n",
    "print(accuracy(test_data['m'], m_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_samples(model, \n",
    "                       x_test=test_data['x'][:25], \n",
    "                       m_test=test_data['m'][:25], \n",
    "                       model_names=model_names,\n",
    "                       figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(test_data['m'], m_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_calibration_error(test_data['m'], m_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overconfidence(test_data['m'], m_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "ms = []\n",
    "\n",
    "for _ in range(10):\n",
    "    test_data = data_gen(n_test, n_obs = 100)\n",
    "    m_pred = model(test_data['x'])['m_probs'].numpy()\n",
    "    \n",
    "    preds.append(m_pred)\n",
    "    ms.append(test_data['m'].numpy())\n",
    "    \n",
    "preds = np.concatenate(preds, axis=0)\n",
    "ms = np.concatenate(ms, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas, accuracies, cal_error = classification_calibration(ms, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot(alphas[1:-1], accuracies)\n",
    "ax.plot(ax.get_xlim(), ax.get_xlim(), '--', color='black')\n",
    "\n",
    "print(overconfidence(ms, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with other NN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 2\n",
    "iterations_per_epoch = 10\n",
    "n_obs_min = 20\n",
    "n_obs_max = 200\n",
    "n_test = 200\n",
    "alpha_smooth = None\n",
    "starter_learning_rate = 0.0005\n",
    "decay_steps = 1000\n",
    "decay_rate = .99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competitor models\n",
    "models = {\n",
    "    'evidential': {\n",
    "        'model': DeepEvidentialModel(EVIDENTIAL_HH),\n",
    "        'checkpoint': 'epidemiology_model_selection_evidentialcompare',\n",
    "        'train_fn': partial(train_online, loss_fun=partial(log_loss, lambd=0.01), method='evidence')\n",
    "    },\n",
    "    \n",
    "    'softmax': {\n",
    "        'model': SoftmaxModel(SOFTMAX_HH),\n",
    "        'checkpoint': \"epidemiology_model_selection_softmax\",\n",
    "        'train_fn': train_online_softmax\n",
    "    },\n",
    "    \n",
    "    'dropout': {\n",
    "        'model': MCDropOutModel(DROPOUT_HH),\n",
    "        'checkpoint': 'epidemiology_model_selection_dropout',\n",
    "        'train_fn': train_online_dropout\n",
    "    },\n",
    "    \n",
    "    'infovae': {\n",
    "        'model': VAE(VAE_HH),\n",
    "        'checkpoint': \"epidemiology_model_selection_infovae\",\n",
    "        'train_fn': partial(train_online_vae, regularization='MMD', regularization_weight=0.1, clip_method='value')\n",
    "    },\n",
    "    \n",
    "    'vae': {\n",
    "        'model': VAE(VAE_HH),\n",
    "        'checkpoint': \"epidemiology_model_selection_vae\",\n",
    "        'train_fn': partial(train_online_vae, regularization='KL', regularization_weight=0.1, clip_method='value')\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Generative model hyperparameters ---#\n",
    "model_names = ['SIR', 'SIRS-VD', 'SIRS-IR']\n",
    "\n",
    "param_priors = [SIR_basic_prior, \n",
    "                SIRS_vitalDynamics_prior, \n",
    "                SIRS_IncidenceRate_prior]\n",
    "\n",
    "forward_models = [SIR_basic, \n",
    "                  SIRS_vitalDynamics, \n",
    "                  SIRS_IncidenceRate]\n",
    "\n",
    "\n",
    "n_obs_min = 20\n",
    "n_obs_max = 200\n",
    "data_gen = partial(data_generator, \n",
    "                   model_prior=model_prior, \n",
    "                   param_priors=param_priors,\n",
    "                   forward_models=forward_models,\n",
    "                   n_obs_min=n_obs_min,\n",
    "                   n_obs_max=n_obs_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "\n",
    "# Train all models\n",
    "for model_name, model in models.items():\n",
    "    \n",
    "    \n",
    "    # Create optimizer\n",
    "    global_step = tfe.Variable(0, dtype=tf.int32)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate,\n",
    "                                               global_step, decay_steps, decay_rate, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Create checkpoint manager\n",
    "    checkpoint = tf.train.Checkpoint(step=global_step, \n",
    "                                     optimizer=optimizer, \n",
    "                                     net=model['model'])\n",
    "    manager = tf.train.CheckpointManager(checkpoint, \n",
    "                                         './checkpoints/{}'.format(model['checkpoint']), \n",
    "                                         max_to_keep=5)\n",
    "    \n",
    "    if manager.latest_checkpoint:\n",
    "        continue\n",
    "    \n",
    "    print('Starting', model_name)\n",
    "    start = timeit.default_timer()\n",
    "    # Loop for each epoch\n",
    "    for ep in range(1, epochs+1):\n",
    "        with tqdm(total=iterations_per_epoch, desc='{}, Training epoch {}'.format(model_name.capitalize(), ep)) as p_bar:\n",
    "            losses = model['train_fn'](model=model['model'], \n",
    "                                       optimizer=optimizer, \n",
    "                                       data_gen=data_gen, \n",
    "                                       iterations=iterations_per_epoch,\n",
    "                                       batch_size=batch_size,\n",
    "                                       p_bar=p_bar,\n",
    "                                       global_step=global_step)\n",
    "            manager.save()\n",
    "    stop = timeit.default_timer()\n",
    "    times.append(stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    \n",
    "    \n",
    "    # Create optimizer\n",
    "    global_step = tfe.Variable(0, dtype=tf.int32)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate,\n",
    "                                               global_step, decay_steps, decay_rate, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Create checkpoint manager\n",
    "    checkpoint = tf.train.Checkpoint(step=global_step, \n",
    "                                     optimizer=optimizer, \n",
    "                                     net=model['model'])\n",
    "    manager = tf.train.CheckpointManager(checkpoint, \n",
    "                                         './checkpoints/{}'.format(model['checkpoint']), \n",
    "                                         max_to_keep=2)\n",
    "    \n",
    "    checkpoint.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_data = data_gen(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chunks = 10 # do not predict at once so not to exhaust GPU memory\n",
    "metrics = {}\n",
    "for model_name, model in models.items():\n",
    "    \n",
    "    # Prepare model metrics\n",
    "    metrics[model_name] = {}\n",
    "    \n",
    "    # Obtain predictions\n",
    "    \n",
    "    m_p = np.concatenate([model['model'].predict(x, to_numpy=True)['m_probs'] \n",
    "                          for x in tf.split(test_data['x'], n_chunks, axis=0)])\n",
    "    \n",
    "    \n",
    "    acc = accuracy(test_data['m'], m_p)\n",
    "    cal_errs, cal_probs = expected_calibration_error(test_data['m'], m_p)\n",
    "    oc = overconfidence(test_data['m'], m_p)\n",
    "        \n",
    "    metrics[model_name]['accuracy'] = acc\n",
    "    metrics[model_name]['calibration'] = cal_errs\n",
    "    metrics[model_name]['overconfidence'] = oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "199.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
